{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pickle import load\n",
    "from argparse import ArgumentParser\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from math import exp, log, floor, ceil\n",
    "from igraph import Graph\n",
    "\n",
    "NUM_ROW = 50000000\n",
    "PERCENT = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 7 # how far a period to look ahead for\n",
    "PAST_WINDOW = 3 # how far back to look for infection status\n",
    "\n",
    "def write_vertex_features(vertex_features, subgraph, age_map):\n",
    "    # for each node in subgraph, write features to vertex_features\n",
    "    # age\n",
    "    # coreness\n",
    "    # authority\n",
    "    # rareness\n",
    "    subgraph_pids = subgraph.vs[\"name\"]\n",
    "    \n",
    "    coreness = subgraph.coreness()\n",
    "    authority = subgraph.authority_score(weights=\"duration\")\n",
    "    degrees = np.array([subgraph.degree(i) for i in range(len(subgraph_pids))])\n",
    "\n",
    "    zero_degrees = degrees == 0\n",
    "    degrees[zero_degrees] = 1\n",
    "    rareness = 1.0/degrees\n",
    "    rareness[zero_degrees] = 0\n",
    "    \n",
    "    vertex_features[tuple(subgraph_pids), 0] = [age_map[pid] for pid in subgraph_pids]\n",
    "    vertex_features[tuple(subgraph_pids), 1] = [coreness[subgraph.vs.find(name=pid).index] for pid in subgraph_pids]\n",
    "    vertex_features[tuple(subgraph_pids), 2] = [authority[subgraph.vs.find(name=pid).index] for pid in subgraph_pids]\n",
    "\n",
    "    vertex_features[tuple(subgraph_pids), 3] = rareness\n",
    "    \n",
    "    return vertex_features\n",
    "\n",
    "def process_subgraph(subgraph, si_table, age_map, vertex_features, ego_pid, cutoff=None, max_size=50):\n",
    "\n",
    "    # for each infected node in subgraph create\n",
    "    # a positive instance, at time t-delta\n",
    "\n",
    "    # create a negative instance for neighbors not infected at time - delta\n",
    "\n",
    "    subgraph_pids = subgraph.vs[\"name\"]\n",
    "    si_subgraph = si_table[si_table.index.get_level_values(\"pid\").isin(subgraph_pids)]\n",
    "\n",
    "    if len(si_subgraph) == 0:\n",
    "        return None\n",
    "\n",
    "    pid_arr = np.pad(np.array(subgraph_pids), (0, max_size-len(subgraph_pids)), \"constant\", constant_values=0)\n",
    "    valid_days = si_subgraph.index.get_level_values(\"infected\")\n",
    "    if cutoff:\n",
    "        valid_days = valid_days[valid_days >= cutoff]\n",
    "\n",
    "    n = len(valid_days.unique())\n",
    "\n",
    "    labels = np.zeros((n,), dtype=int)\n",
    "    influence_feature = np.zeros((n,max_size,2))\n",
    "    adjacency_matrix = np.zeros((n,max_size, max_size))\n",
    "    vertex_id = np.zeros((n,max_size), dtype=int)\n",
    "\n",
    "    subgraph_adj = np.array(subgraph.get_adjacency().data)\n",
    "    pad_num = max_size - subgraph_adj.shape[0]  \n",
    "    adj = np.pad(subgraph_adj, (0,pad_num), \"constant\", constant_values=0)\n",
    "    \n",
    "    vertex_features = write_vertex_features(vertex_features, subgraph, age_map)\n",
    "    \n",
    "    label_idx = 0\n",
    "\n",
    "    for day in valid_days.unique():\n",
    "        # look forward delta steps to identify \n",
    "        min_day = day - PAST_WINDOW\n",
    "\n",
    "        day_level = si_subgraph.index.get_level_values(\"infected\").to_series()\n",
    "        back_subgraph = si_subgraph[day_level.between(min_day, day).values]\n",
    "    \n",
    "        state_t_d = si_subgraph[day_level.between(day, day+DELTA, inclusive=\"right\").values]\n",
    "\n",
    "        labels[label_idx] = ego_pid in state_t_d.index.get_level_values(\"pid\")\n",
    "        adjacency_matrix[label_idx] = adj\n",
    "        vertex_id[label_idx] = pid_arr\n",
    "\n",
    "        influence_feature[label_idx,:,0] = (pid_arr == ego_pid).astype(int)\n",
    "\n",
    "        # get neighbors who are infected at time t-delta \n",
    "        is_inf = np.isin(pid_arr, back_subgraph.index.get_level_values(\"pid\").unique()).astype(int)\n",
    "        influence_feature[label_idx,:,1] = is_inf\n",
    "       \n",
    "        label_idx += 1\n",
    " \n",
    "    return (labels, adjacency_matrix, influence_feature, vertex_id, vertex_features)\n",
    "\n",
    "def rand_walk_igraph(graph, start_node, size, restart_prob, wname=\"weight\"):\n",
    "    \"\"\"\n",
    "    Do an edge-weighted random walk with restart starting from start_node\n",
    "    on graph.\n",
    "\n",
    "    graph - an igraph graph with vertex attributes \"name\" and edge attributes wname\n",
    "    start_node - a vertex in graph (note that this *should not* be the vertex name)\n",
    "    size - the maximum size of the random walk\n",
    "    restart_prob - the probability of returning to start_node at each step\n",
    "    wname (optional) - the name of the edge attribute to weight\n",
    "\n",
    "    returns - subgraph (an igraph graph produced through the random walk)\n",
    "    \"\"\"\n",
    "    nodes = set([start_node])\n",
    "    current = start_node\n",
    "\n",
    "    # To avoid a situation where we end up in a loop, we limit the number\n",
    "    # of steps we can take without adding a new node\n",
    "\n",
    "    max_step = 200\n",
    "    step = 0\n",
    "\n",
    "    while len(nodes) < size:\n",
    "        curr_size = len(nodes) # TODO: preprocess pid_part to ensure degree > 0 => avoid max_step iterations\n",
    "        if random.random() < restart_prob or len(graph.neighbors(current)) == 0:\n",
    "            current = start_node\n",
    "        else:\n",
    "            poss_edges = graph.incident(current)\n",
    "            poss_weights = np.array([graph.es[n][wname] for n in poss_edges])\n",
    "            if len(poss_edges) == 0:\n",
    "                None\n",
    "                # print(f\"There are no incident edges for {current}, but neighbors {graph.neighbors(current)}\")\n",
    "            if poss_weights.sum() == 0:\n",
    "                # print(f\"Edge weights are 0 for {current}\")\n",
    "                current = start_node\n",
    "                # TODO: make into exception\n",
    "            else:\n",
    "                #poss_weights = np.nan_to_num(poss_weights)\n",
    "                new_edge = np.random.choice(poss_edges, p = poss_weights/poss_weights.sum())\n",
    "\n",
    "                if graph.es[new_edge].source == current:\n",
    "                    current = graph.es[new_edge].target\n",
    "                else:\n",
    "                    current = graph.es[new_edge].source \n",
    "            nodes.add(current)\n",
    "        if len(nodes) == curr_size:\n",
    "            step += 1\n",
    "        else:\n",
    "            step = 0\n",
    "        if step == max_step:\n",
    "            return graph.induced_subgraph(list(nodes))\n",
    "    return graph.induced_subgraph(list(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_outcome_data_path = './data/pandemic/centralized/train/va_disease_outcome_training.csv.gz'\n",
    "person_data_path = './data/pandemic/centralized/train/va_person.csv.gz'\n",
    "population_network_data_path = './data/pandemic/centralized/train/va_population_network.csv.gz'\n",
    "\n",
    "disease_outcome_df = pd.read_csv(disease_outcome_data_path, compression='gzip', nrows=NUM_ROW)\n",
    "person_df = pd.read_csv(person_data_path, compression='gzip', nrows=NUM_ROW)\n",
    "population_network_df = pd.read_csv(population_network_data_path, compression='gzip', nrows=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made age map, 7688058 is the max_id\n"
     ]
    }
   ],
   "source": [
    "# read in population file (person_df)\n",
    "pop_file = person_df\n",
    "age_map = pop_file[[\"pid\", \"age\"]].set_index(\"pid\")[\"age\"].to_dict()\n",
    "max_id = max(age_map.keys())\n",
    "print(f\"Made age map, {max_id} is the max_id\")\n",
    "vertex_features = np.zeros((max_id+1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in subgraph list (disease_outcome_df)\n",
    "disease_data = disease_outcome_df\n",
    "is_infected = disease_data[\"state\"] == \"I\"\n",
    "pid = disease_data[is_infected][\"pid\"]\n",
    "day = disease_data[is_infected][\"day\"]\n",
    "\n",
    "inf_time_df = pd.DataFrame({\"pid\" : pid, \"day\" : day})\n",
    "\n",
    "is_rec = disease_data[\"state\"] == \"R\"\n",
    "pid = disease_data[is_rec][\"pid\"]\n",
    "day = disease_data[is_rec][\"day\"]\n",
    "\n",
    "rec_time_df = pd.DataFrame({\"pid\" : pid, \"day\" : day})\n",
    "\n",
    "def lookup_rec(row):\n",
    "    pid_subset = rec_time_df[(rec_time_df[\"pid\"] == row[\"pid\"]) & (rec_time_df[\"day\"] >= row[\"day\"])]\n",
    "    if pid_subset.shape[0] > 0:\n",
    "        return pid_subset[\"day\"].min()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "recovery_times = inf_time_df.apply(lookup_rec, axis=1)\n",
    "si_table = inf_time_df\n",
    "si_table.rename({\"day\" : \"infected\"}, axis=1, inplace=True)\n",
    "\n",
    "si_table[\"recovery\"] = recovery_times\n",
    "\n",
    "si_table.set_index([\"pid\", \"infected\"], verify_integrity=True, inplace=True)\n",
    "si = si_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (population_network_df)\n",
    "edges = population_network_df\n",
    "edges[\"pid1s\"] = edges.apply(lambda row: min(row[\"pid1\"], row[\"pid2\"]), axis=1)\n",
    "edges[\"pid2s\"] = edges.apply(lambda row: max(row[\"pid1\"], row[\"pid2\"]), axis=1)\n",
    "\n",
    "edges = edges.drop(columns=['pid1', 'pid2'])\n",
    "edges = edges.rename(columns={'pid1s':'pid1', 'pid2s':'pid2'})\n",
    "\n",
    "collapsed_edges = edges[[\"pid1\", \"pid2\", \"duration\"]].groupby([\"pid1\", \"pid2\"]).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17749 vertices\n",
      "There are 17749 subgraphs\n"
     ]
    }
   ],
   "source": [
    "graph_df = collapsed_edges.reset_index()\n",
    "graph_df.head()\n",
    "tuples = [tuple(x) for x in graph_df.values]\n",
    "graph = Graph.TupleList(tuples, directed = True, edge_attrs = ['duration'])\n",
    "vertex_names = list(graph.vs[\"name\"])\n",
    "vertex_names.sort()\n",
    "\n",
    "print(f\"There are {len(vertex_names)} vertices\")\n",
    "graph_dict = {}\n",
    "n_dict = 0\n",
    "\n",
    "pids = vertex_names\n",
    "node = []\n",
    "for pid in pids:\n",
    "    node = graph.vs.find(name=pid) # name\n",
    "    subgraph = rand_walk_igraph(graph, node, 50, 0.8, wname=\"duration\")\n",
    "    graph_dict[pid] = subgraph\n",
    "\n",
    "subgraphs = graph_dict\n",
    "print(f\"There are {len(subgraphs)} subgraphs\")\n",
    "sample_size = 300000\n",
    "index_set = np.random.choice(list(subgraphs.keys()), replace=False, size=min(sample_size, len(subgraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subgraphs: 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_list = []\n",
    "adj_list = []\n",
    "inf_list = []\n",
    "vert_id = []\n",
    "for i,subgraph_pid in enumerate(index_set): \n",
    "\n",
    "    subgraph = subgraphs[subgraph_pid]\n",
    "    output = process_subgraph(subgraph, si, age_map, vertex_features, subgraph_pid, None)\n",
    "\n",
    "    if output is not None:\n",
    "        labels, adjacency_matrix, influence_feature, vertex_id, vertex_features = output\n",
    "\n",
    "        label_list.append(labels)\n",
    "        adj_list.append(adjacency_matrix)\n",
    "        inf_list.append(influence_feature)\n",
    "        vert_id.append(vertex_id)\n",
    "\n",
    "    # if (i+1) % 100 == 0:\n",
    "    #     print(f\"Handled {i}/{len(index_set)} instances, {len(label_list)} added\")\n",
    "print(f\"Total subgraphs: {len(label_list)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6df1f215c9f8d9ecab2d2b3cd12e0d23b51c9c76521b67c5d4d6f98271abbbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
